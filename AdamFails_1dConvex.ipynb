{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMS paper claims that...\n",
    "\n",
    "ADAM iterates $x_t$ converge to x = 1, which unfortunately has the largest regret amongst all points in the domain. On the other hand, the average regret of AMSGRAD converges to 0 and its iterate converges to the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "  f_t(x)=\\begin{cases}\n",
    "    1010x, & \\text{with probability 0.01}\\\\\n",
    "    -10x, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where $x=[-1, 1]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, Let's implement ADAM from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class ADAMOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    implements ADAM Algorithm, as a preceding step.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super(ADAMOptimizer, self).__init__(params, defaults)\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        for group in self.param_groups:\n",
    "            #print(group.keys())\n",
    "            #print (self.param_groups[0]['params'][0].size()), First param (W) size: torch.Size([10, 784])\n",
    "            #print (self.param_groups[0]['params'][1].size()), Second param(b) size: torch.Size([10])\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Momentum (Exponential MA of gradients)\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    #print(p.data.size())\n",
    "                    # RMS Prop componenet. (Exponential MA of squared gradients). Denominator.\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    \n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "                b1, b2 = group['betas']\n",
    "                state['step'] += 1\n",
    "                \n",
    "                # L2 penalty. Gotta add to Gradient as well.\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Momentum\n",
    "                exp_avg = torch.mul(exp_avg, b1) + (1 - b1)*grad\n",
    "                # RMS\n",
    "                exp_avg_sq = torch.mul(exp_avg_sq, b2) + (1-b2)*(grad*grad)\n",
    "                \n",
    "                denom = exp_avg_sq.sqrt() + group['eps']\n",
    "\n",
    "                bias_correction1 = 1 / (1 - b1 ** state['step'])\n",
    "                bias_correction2 = 1 / (1 - b2 ** state['step'])\n",
    "                \n",
    "                adapted_learning_rate = group['lr'] * bias_correction1 / math.sqrt(bias_correction2)\n",
    "\n",
    "                p.data = p.data - adapted_learning_rate * exp_avg / denom \n",
    "                \n",
    "                if state['step']  % 10000 ==0:\n",
    "                    print (\"group:\", group)\n",
    "                    print(\"p: \",p)\n",
    "                    print(\"p.data: \", p.data) # W = p.data\n",
    "                \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second, Implement AMS on top of ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing both ADAM & AMS on simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Hyper-parameters as stated in the paper - https://openreview.net/forum?id=ryQu7f-RZ\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "# Loss: nn.CrossEntropyLoss() computes softmax internally\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "# Adam\n",
    "#optimizer = ADAMOptimizer(model.parameters(), lr=learning_rate)  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.99), eps=1e-8, weight_decay=0) # To double check the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Linear(in_features=784, out_features=10, bias=True)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/469], Loss: 2.3474\n",
      "Epoch [1/5], Step [101/469], Loss: 0.7269\n",
      "Epoch [1/5], Step [201/469], Loss: 0.5375\n",
      "Epoch [1/5], Step [301/469], Loss: 0.4355\n",
      "Epoch [1/5], Step [401/469], Loss: 0.4653\n",
      "Epoch [2/5], Step [1/469], Loss: 0.3485\n",
      "Epoch [2/5], Step [101/469], Loss: 0.3062\n",
      "Epoch [2/5], Step [201/469], Loss: 0.3113\n",
      "Epoch [2/5], Step [301/469], Loss: 0.3699\n",
      "Epoch [2/5], Step [401/469], Loss: 0.3680\n",
      "Epoch [3/5], Step [1/469], Loss: 0.2570\n",
      "Epoch [3/5], Step [101/469], Loss: 0.3891\n",
      "Epoch [3/5], Step [201/469], Loss: 0.2187\n",
      "Epoch [3/5], Step [301/469], Loss: 0.3937\n",
      "Epoch [3/5], Step [401/469], Loss: 0.2861\n",
      "Epoch [4/5], Step [1/469], Loss: 0.1906\n",
      "Epoch [4/5], Step [101/469], Loss: 0.3923\n",
      "Epoch [4/5], Step [201/469], Loss: 0.2604\n",
      "Epoch [4/5], Step [301/469], Loss: 0.3092\n",
      "Epoch [4/5], Step [401/469], Loss: 0.2939\n",
      "Epoch [5/5], Step [1/469], Loss: 0.2515\n",
      "Epoch [5/5], Step [101/469], Loss: 0.2943\n",
      "Epoch [5/5], Step [201/469], Loss: 0.2574\n",
      "Epoch [5/5], Step [301/469], Loss: 0.3118\n",
      "Epoch [5/5], Step [401/469], Loss: 0.2775\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "total_step = len(train_loader)\n",
    "adam_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        adam_loss_list.append(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, batch_idx+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wFtsBU/S2UTwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a93a5054e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(adam_loss_list, label=\"adam\")\n",
    "#plt.plot(results_amsgrad, label=\"amsgrad\")\n",
    "plt.legend(bbox_to_anchor=(0.8, 0.9), loc=2, borderaxespad=0.)\n",
    "#plt.savefig(\"fig1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 91 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Synthetic experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_max = Variable(torch.Tensor([1.0]), requires_grad=True)\n",
    "x_min = Variable(torch.Tensor([-1.0]), requires_grad=True)\n",
    "\n",
    "def online_f_t(x, t):\n",
    "    if t % 101 == 1:\n",
    "        return 1010.0*x\n",
    "    else:\n",
    "        return -10.0*x\n",
    "    \n",
    "def f_min(t):\n",
    "    if t % 101 == 1:\n",
    "        return -1010.0\n",
    "    else:\n",
    "        return 10.0\n",
    "\n",
    "def domain_constraints(x):\n",
    "    if x > 1.0:\n",
    "        return x_max\n",
    "    if x < -1.0:\n",
    "        return x_min\n",
    "    return x\n",
    "\n",
    "\n",
    "def OnlineLearning(learning_rate=1e-3, amsgrad=False):\n",
    "    x = Variable(torch.Tensor([0.5]), requires_grad=True)\n",
    "    optimizer = ADAMOptimizer([x], lr=learning_rate, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "    regret_sum = 0\n",
    "    time_steps = []\n",
    "    avg_regret_history = []\n",
    "    x_history = []\n",
    "\n",
    "    for step in range(1,1000001):\n",
    "        x = domain_constraints(x)\n",
    "        loss = online_f_t(x, step)\n",
    "        # This is how the regret is defined in this paper.\n",
    "        regret_sum += (loss.item() - f_min(step))\n",
    "        regret_avg = regret_sum/step\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        \"\"\"\n",
    "        if step%10000 == 0:\n",
    "            time_steps.append(step)\n",
    "            avg_regret_history.append(regret_avg)\n",
    "            x_history.append(x.item())\n",
    "            print ('step : ',step,  '  x : ',x, '  loss : ',loss, '   regret_sum : ',regret_sum, ' regret_avg : ',regret_avg )\n",
    "            print (x.grad)\n",
    "        \"\"\"\n",
    "        optimizer.step()   \n",
    "    return time_steps, avg_regret_history, x_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-cbef92e01a8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# AMSGrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_regret_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOnlineLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-50ae60c4aa0d>\u001b[0m in \u001b[0;36mOnlineLearning\u001b[1;34m(learning_rate, amsgrad)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# calculate gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \"\"\"\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-1\n",
    "# AMSGrad\n",
    "time_steps, avg_regret_history, x_history = OnlineLearning(learning_rate=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
