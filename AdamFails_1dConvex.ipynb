{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMS paper claims that...\n",
    "\n",
    "ADAM iterates $x_t$ converge to x = 1, which unfortunately has the largest regret amongst all points in the domain. On the other hand, the average regret of AMSGRAD converges to 0 and its iterate converges to the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "  f_t(x)=\\begin{cases}\n",
    "    1010x, & \\text{with probability 0.01}\\\\\n",
    "    -10x, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where $x=[-1, 1]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets verify if it's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, Let's implement ADAM from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class ADAMOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    implements ADAM Algorithm, as a preceding step.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super(ADAMOptimizer, self).__init__(params, defaults)\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        for group in self.param_groups:\n",
    "            #print (self.param_groups[0]['params'][0].size()), First param (W) size: torch.Size([10, 784])\n",
    "            #print (self.param_groups[0]['params'][1].size()), Second param(b) size: torch.Size([10])\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Momentum (Exponential MA of gradients)\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    #print(p.data.size())\n",
    "                    # RMS Prop componenet. (Exponential MA of squared gradients). Denominator.\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    \n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "                b1, b2 = group['betas']\n",
    "                state['step'] += 1\n",
    "                \n",
    "                # L2 penalty. Gotta add to Gradient as well.\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Momentum\n",
    "                exp_avg = torch.mul(exp_avg, b1) + (1 - b1)*grad\n",
    "                # RMS\n",
    "                exp_avg_sq = torch.mul(exp_avg_sq, b2) + (1-b2)*(grad*grad)\n",
    "                \n",
    "                denom = exp_avg_sq.sqrt() + group['eps']\n",
    "\n",
    "                bias_correction1 = 1 / (1 - b1 ** state['step'])\n",
    "                bias_correction2 = 1 / (1 - b2 ** state['step'])\n",
    "                \n",
    "                adapted_learning_rate = group['lr'] * bias_correction1 / math.sqrt(bias_correction2)\n",
    "\n",
    "                p.data = p.data - adapted_learning_rate * exp_avg / denom \n",
    "                \n",
    "                if state['step']  % 10000 ==0:\n",
    "                    print (\"group:\", group)\n",
    "                    print(\"p: \",p)\n",
    "                    print(\"p.data: \", p.data) # W = p.data\n",
    "                \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second, Implement AMS on top of ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing both ADAM & AMS on simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Hyper-parameters as stated in the paper - https://openreview.net/forum?id=ryQu7f-RZ\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "# Loss: nn.CrossEntropyLoss() computes softmax internally\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "# Adam\n",
    "optimizer = ADAMOptimizer(model.parameters(), lr=learning_rate)  \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.99), eps=1e-8, weight_decay=0) # To double check the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Linear(in_features=784, out_features=10, bias=True)>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/469], Loss: 2.3241\n",
      "Epoch [1/5], Step [101/469], Loss: 0.8930\n",
      "Epoch [1/5], Step [201/469], Loss: 0.4822\n",
      "Epoch [1/5], Step [301/469], Loss: 0.4837\n",
      "Epoch [1/5], Step [401/469], Loss: 0.3685\n",
      "Epoch [2/5], Step [1/469], Loss: 0.4409\n",
      "Epoch [2/5], Step [101/469], Loss: 0.4088\n",
      "Epoch [2/5], Step [201/469], Loss: 0.4610\n",
      "Epoch [2/5], Step [301/469], Loss: 0.3673\n",
      "Epoch [2/5], Step [401/469], Loss: 0.2353\n",
      "Epoch [3/5], Step [1/469], Loss: 0.4252\n",
      "Epoch [3/5], Step [101/469], Loss: 0.3476\n",
      "Epoch [3/5], Step [201/469], Loss: 0.5284\n",
      "Epoch [3/5], Step [301/469], Loss: 0.4335\n",
      "Epoch [3/5], Step [401/469], Loss: 0.2737\n",
      "Epoch [4/5], Step [1/469], Loss: 0.3128\n",
      "Epoch [4/5], Step [101/469], Loss: 0.3986\n",
      "Epoch [4/5], Step [201/469], Loss: 0.2595\n",
      "Epoch [4/5], Step [301/469], Loss: 0.3329\n",
      "Epoch [4/5], Step [401/469], Loss: 0.2046\n",
      "Epoch [5/5], Step [1/469], Loss: 0.5740\n",
      "Epoch [5/5], Step [101/469], Loss: 0.4283\n",
      "Epoch [5/5], Step [201/469], Loss: 0.3684\n",
      "Epoch [5/5], Step [301/469], Loss: 0.6881\n",
      "Epoch [5/5], Step [401/469], Loss: 0.2965\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "total_step = len(train_loader)\n",
    "adam_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        adam_loss_list.append(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, batch_idx+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZkQgzsRkQn9P5AV4ZsjHTEzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e5c1d326a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(adam_loss_list, label=\"adam\")\n",
    "#plt.plot(results_amsgrad, label=\"amsgrad\")\n",
    "plt.legend(bbox_to_anchor=(0.8, 0.9), loc=2, borderaxespad=0.)\n",
    "#plt.savefig(\"fig1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 91 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Synthetic experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = Variable(torch.Tensor([1.0]), requires_grad=True)\n",
    "x_min = Variable(torch.Tensor([-1.0]), requires_grad=True)\n",
    "\n",
    "def online_f_t(x, t):\n",
    "    if t % 101 == 1:\n",
    "        return 1010.0*x\n",
    "    else:\n",
    "        return -10.0*x\n",
    "    \n",
    "def f_min(t):\n",
    "    if t % 101 == 1:\n",
    "        return -1010.0\n",
    "    else:\n",
    "        return 10.0\n",
    "\n",
    "def domain_constraints(x):\n",
    "    if x > 1.0:\n",
    "        return x_max\n",
    "    if x < -1.0:\n",
    "        return x_min\n",
    "    return x\n",
    "\n",
    "\n",
    "def OnlineLearning(learning_rate=1e-3, amsgrad=False):\n",
    "    x = Variable(torch.Tensor([0.5]), requires_grad=True)\n",
    "    optimizer = ADAMOptimizer([x], lr=learning_rate, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "    regret_sum = 0\n",
    "    time_steps = []\n",
    "    avg_regret_history = []\n",
    "    x_history = []\n",
    "\n",
    "    for step in range(1,1000001):\n",
    "        x = domain_constraints(x)\n",
    "        loss = online_f_t(x, step)\n",
    "        # This is how the regret is defined in this paper.\n",
    "        regret_sum += (loss.item() - f_min(step))\n",
    "        regret_avg = regret_sum/step\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        \"\"\"\n",
    "        if step%10000 == 0:\n",
    "            time_steps.append(step)\n",
    "            avg_regret_history.append(regret_avg)\n",
    "            x_history.append(x.item())\n",
    "            print ('step : ',step,  '  x : ',x, '  loss : ',loss, '   regret_sum : ',regret_sum, ' regret_avg : ',regret_avg )\n",
    "            print (x.grad)\n",
    "        \"\"\"\n",
    "        optimizer.step()   \n",
    "    return time_steps, avg_regret_history, x_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n",
      "group: {'params': [tensor([ 1.0365])], 'lr': 0.1, 'betas': (0.9, 0.99), 'eps': 1e-08, 'weight_decay': 0}\n",
      "p:  tensor([ 1.0365])\n",
      "p.data:  tensor([ 1.0365])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-cbef92e01a8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# AMSGrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_regret_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOnlineLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-50ae60c4aa0d>\u001b[0m in \u001b[0;36mOnlineLearning\u001b[1;34m(learning_rate, amsgrad)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# calculate gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \"\"\"\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-1\n",
    "# AMSGrad\n",
    "time_steps, avg_regret_history, x_history = OnlineLearning(learning_rate=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
